# LLaMA Monitoring Server - Log Event Documentation

This document provides a comprehensive guide to understanding the four types of log events generated by the LLaMA monitoring server. Each event is logged in JSON format with specific structures and contains essential information for monitoring the inference process.

## Overview

The monitoring system logs four distinct event types during the inference process:
1. **session_start** - Marks the beginning of a new inference session
2. **model_metrics** - Provides comprehensive model and context information
3. **sampling_state** - Captures detailed token generation state (most frequent event)
4. **session_end** - Marks the completion of an inference session

All events share common fields:
- `event`: The event type identifier
- `timestamp`: ISO 8601 timestamp when the event occurred
- `session_id`: Unique identifier for the inference session

---

## Event Type 1: session_start

**Purpose**: Marks the beginning of a new inference session and captures the input prompt.

**Structure**:
```json
{
  "event": "session_start",
  "timestamp": "2025-08-22 23:34:41.435",
  "session_id": "sess_25e52044",
  "prompt": "Tell me about deep learning"
}
```

**Fields**:
- `event`: Always "session_start"
- `timestamp`: Session initialization timestamp
- `session_id`: Unique session identifier (format: sess_XXXXXXXX)
- `prompt`: The input text prompt provided by the user

**Use Cases**:
- Session tracking and correlation
- Understanding user queries
- Performance analysis initialization

---

## Event Type 2: model_metrics

**Purpose**: Provides comprehensive information about the model architecture, context configuration, and memory usage. This is logged once per session immediately after session_start.

**Structure**:
```json
{
  "event": "model_metrics",
  "timestamp": "2025-08-22 23:34:41.435",
  "session_id": "sess_25e52044",
  "model_info": {
    "n_vocab": 262144,
    "n_ctx_train": 32768,
    "n_embd": 1152,
    "n_layer": 26,
    "n_head": 4,
    "n_head_kv": 1,
    "model_size_mb": 762
  },
  "context_info": {
    "n_ctx": 2048,
    "n_batch": 64,
    "n_ubatch": 64,
    "n_seq_max": 1
  },
  "memory_usage": {
    "model_size_bytes": 799525120,
    "context_size_estimate_bytes": 490733568
  }
}
```

**Fields**:

### model_info
- `n_vocab`: Size of the vocabulary (total number of tokens the model can handle)
- `n_ctx_train`: Maximum context length the model was trained on
- `n_embd`: Embedding dimension (hidden size of the model)
- `n_layer`: Number of transformer layers in the model
- `n_head`: Number of attention heads per layer
- `n_head_kv`: Number of key-value attention heads (for grouped-query attention)
- `model_size_mb`: Approximate model size in megabytes

### context_info
- `n_ctx`: Current context window size being used
- `n_batch`: Batch size for processing
- `n_ubatch`: Micro-batch size for processing
- `n_seq_max`: Maximum number of sequences that can be processed simultaneously

### memory_usage
- `model_size_bytes`: Exact memory footprint of the model in bytes
- `context_size_estimate_bytes`: Estimated memory required for context processing

**Use Cases**:
- Model performance analysis
- Memory optimization
- Architecture understanding
- Debugging context limitations

---

## Event Type 3: sampling_state

**Purpose**: The core monitoring event that captures detailed information about each token generation step. This event provides deep insights into the model's decision-making process.

**Structure**:
```json
{
  "event": "sampling_state",
  "timestamp": "2025-08-22 23:34:41.524",
  "sampling": {
    "logits_sample": [19.627302, 19.400139, 19.032385, 18.765900, 18.073578, 16.789043, 16.757132, 16.471830, 16.452129, 16.410023],
    "top_tokens": [236881, 4681, 236761, 532, 236764, 573, 17927, 22823, 2028, 107],
    "top_probs": [0.310355, 0.247288, 0.171195, 0.131147, 0.065627, 0.018164, 0.017594, 0.013227, 0.012969, 0.012434],
    "top_token_texts": ["?", " models", ".", " and", ",", " for", " algorithms", " neural", " model", "\n"],
    "selected_token": 236881,
    "selected_prob": 0.310355,
    "sampling_method": "greedy",
    "sampling_params": {},
    "layer_details": [
      {
        "layer_id": 0,
        "layer_type": "attention",
        "operation": "multi_head_self_attention",
        "execution_time_us": 1000,
        "layer_metrics": {
          "attention_heads": 4.0,
          "hidden_dim": 1152.0,
          "intermediate_dim": 0.0
        }
      }
      // ... more layers
    ]
  },
  "session_id": "sess_25e52044"
}
```

### Detailed Field Explanations:

#### Core Sampling Information
- **`logits_sample`** (array of floats): 
  - Raw logit values from the model's output layer for the top candidate tokens
  - Higher values indicate stronger model preference for that token
  - These are pre-softmax values that represent the model's "raw confidence"

- **`top_tokens`** (array of integers):
  - Token IDs of the most likely next tokens (corresponding to logits_sample)
  - These are vocabulary indices that map to actual text representations
  - Ordered by probability (highest to lowest)

- **`top_probs`** (array of floats):
  - Post-softmax probabilities for each candidate token (0.0 to 1.0)
  - Sum of all probabilities in the full vocabulary equals 1.0
  - These show the actual likelihood the model assigns to each token

- **`top_token_texts`** (array of strings):
  - Human-readable text representation of each candidate token
  - Includes special formatting (spaces, punctuation, newlines)
  - Direct mapping from `top_tokens` through the tokenizer

- **`selected_token`** (integer):
  - The actual token ID chosen by the sampling algorithm
  - Must be one of the tokens from `top_tokens` array
  - This token will be added to the generated sequence

- **`selected_prob`** (float):
  - The probability of the selected token (0.0 to 1.0)
  - Matches the corresponding probability in `top_probs`
  - Higher values indicate more confident selections

#### Sampling Configuration
- **`sampling_method`** (string):
  - The algorithm used for token selection
  - Common values: "greedy", "top_k", "top_p", "temperature"
  - "greedy" always selects the highest probability token

- **`sampling_params`** (object):
  - Parameters specific to the sampling method used
  - For greedy sampling: typically empty {}
  - For temperature sampling: {"temperature": 0.8}
  - For top-k: {"top_k": 40}
  - For top-p: {"top_p": 0.9}

#### Layer-by-Layer Performance Details
- **`layer_details`** (array of objects):
  Each layer object contains:
  
  - **`layer_id`** (integer): Zero-indexed layer number (0 to n_layer-1)
  
  - **`layer_type`** (string): Type of neural network layer
    - "attention": Multi-head self-attention layer
    - "feed_forward": Feed-forward network (MLP) layer
  
  - **`operation`** (string): Specific operation performed
    - "multi_head_self_attention": Standard transformer attention
    - "mlp_projection": Multi-layer perceptron forward pass
  
  - **`execution_time_us`** (integer): Layer execution time in microseconds
    - Critical for performance profiling
    - Identifies bottleneck layers
    - Helps optimize inference speed
  
  - **`layer_metrics`** (object): Layer-specific computational metrics
    - **`attention_heads`** (float): Number of attention heads (for attention layers)
    - **`hidden_dim`** (float): Hidden dimension size
    - **`intermediate_dim`** (float): Intermediate dimension (feed-forward layers only)

### Sampling State Analysis Use Cases:

#### Token Generation Monitoring
- Track model confidence through probability distributions
- Identify uncertain generations (low max probability)
- Monitor vocabulary usage patterns

#### Performance Profiling
- Identify slow layers through `execution_time_us`
- Analyze per-layer computational cost
- Optimize model deployment based on bottlenecks

#### Quality Assessment
- High `selected_prob` values indicate confident generation
- Diverse `top_probs` suggest creative/uncertain generation
- Repetitive `selected_token` patterns may indicate model issues

#### Debugging and Analysis
- Compare `logits_sample` to understand raw model preferences
- Analyze `top_token_texts` to see alternative generation paths
- Use `layer_details` for detailed performance debugging

---

## Event Type 4: session_end

**Purpose**: Marks the completion of an inference session and provides summary statistics.

**Structure**:
```json
{
  "event": "session_end",
  "timestamp": "2025-08-22 23:34:52.089",
  "session_id": "sess_25e52044",
  "duration_ms": 11653,
  "total_steps": 513,
  "input_token_count": 0,
  "output_token_count": 0
}
```

**Fields**:
- `event`: Always "session_end"
- `timestamp`: Session completion timestamp
- `session_id`: Matching session identifier from session_start
- `duration_ms`: Total inference time in milliseconds (from session_start to session_end)
- `total_steps`: Total number of inference steps (typically equals number of sampling_state events)
- `input_token_count`: Number of tokens in the input prompt (currently 0 in this implementation)
- `output_token_count`: Number of tokens generated as output (currently 0 in this implementation)

**Use Cases**:
- Session performance measurement
- Throughput calculation (tokens/second)
- Resource utilization analysis
- Billing and usage tracking

---

## Log Analysis Best Practices

### Performance Monitoring
1. Monitor `execution_time_us` in `sampling_state` events to identify performance bottlenecks
2. Track `duration_ms` in `session_end` events for overall session performance
3. Calculate tokens per second using `total_steps` and `duration_ms`

### Quality Assessment
1. Monitor `selected_prob` values - consistently low values may indicate poor model fit
2. Analyze `top_probs` distribution - high entropy suggests uncertainty
3. Track `sampling_method` usage for consistency

### Resource Management
1. Use `model_metrics` for memory planning and optimization
2. Monitor `layer_details` execution times for hardware scaling decisions
3. Track session frequency and duration for capacity planning

### Debugging
1. Correlate `session_id` across all event types for complete session analysis
2. Use `layer_details` to identify computational bottlenecks
3. Analyze `top_token_texts` to understand model behavior and potential issues

---

## Log File Structure

- **Location**: `tools/monitoring-server/logs/`
- **Naming**: `sess_YYYYMMDD_HHMMSS_microseconds.log`
- **Format**: One JSON event per line (JSONL format)
- **Encoding**: UTF-8
- **Rotation**: New file per session

This documentation provides the foundation for understanding and analyzing the comprehensive monitoring data generated by the LLaMA inference server.
